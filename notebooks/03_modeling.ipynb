{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2163cf",
   "metadata": {},
   "source": [
    "# 03 — Modelling & Evaluation\n",
    "\n",
    "**MarketPulse Phase 1**\n",
    "\n",
    "This notebook demonstrates the full training pipeline:\n",
    "1. Fetch data → preprocess → features → labels\n",
    "2. Walk-forward cross-validation (no data leakage)\n",
    "3. XGBoost training with class balancing\n",
    "4. Per-fold and aggregate evaluation\n",
    "5. SHAP explainability analysis\n",
    "6. Comparison: 2-class vs 3-class, multiple horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.data.market_config import load_market_config, load_strategy_config\n",
    "from src.data.fetcher import YFinanceFetcher\n",
    "from src.data.preprocessing import preprocess_ohlcv\n",
    "from src.features.technical import compute_technical_indicators\n",
    "from src.features.returns import compute_return_features\n",
    "from src.features.labels import generate_labels, get_clean_features_and_labels\n",
    "from src.utils.validation import WalkForwardValidator\n",
    "from src.models.xgboost_classifier import MarketPulseXGBClassifier\n",
    "from src.models.evaluator import MarketPulseEvaluator\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32127991",
   "metadata": {},
   "source": [
    "## 1. Data Preparation Pipeline\n",
    "\n",
    "Full pipeline: fetch → preprocess → features → labels → clean split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs\n",
    "market_config = load_market_config('stocks')\n",
    "strategy_config = load_strategy_config('short_term')\n",
    "\n",
    "# Fetch data\n",
    "fetcher = YFinanceFetcher(market_config=market_config)\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')\n",
    "\n",
    "raw = fetcher.fetch('AAPL', start=start_date, end=end_date)\n",
    "print(f\"Raw data: {len(raw)} rows\")\n",
    "\n",
    "# Preprocess\n",
    "df = preprocess_ohlcv(raw, market_config=market_config)\n",
    "print(f\"After preprocessing: {len(df)} rows\")\n",
    "\n",
    "# Features\n",
    "df = compute_technical_indicators(df)\n",
    "df = compute_return_features(df)\n",
    "print(f\"After features: {len(df)} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Labels (3-class, 1-day horizon, ±1% threshold)\n",
    "df = generate_labels(df, horizon=1, label_type='classification', num_classes=3, threshold=0.01)\n",
    "\n",
    "# Clean split\n",
    "X, y = get_clean_features_and_labels(df)\n",
    "print(f\"\\nClean dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(y.value_counts().sort_index().rename({0: 'DOWN', 1: 'FLAT', 2: 'UP'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365609d5",
   "metadata": {},
   "source": [
    "## 2. Walk-Forward Validation\n",
    "\n",
    "Unlike standard k-fold, walk-forward validation **never leaks future data** into training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = WalkForwardValidator.from_strategy_config(strategy_config)\n",
    "folds = validator.split(X)\n",
    "print(validator.summary(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbecb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the walk-forward splits\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "for fold in folds:\n",
    "    # Train period\n",
    "    ax.barh(fold.fold_number, fold.train_size, left=fold.train_start,\n",
    "            color='steelblue', alpha=0.7, height=0.8)\n",
    "    # Test period\n",
    "    ax.barh(fold.fold_number, fold.test_size, left=fold.test_start,\n",
    "            color='coral', alpha=0.9, height=0.8)\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Fold Number')\n",
    "ax.set_title('Walk-Forward Validation — Expanding Window')\n",
    "ax.legend(['Train', 'Test'], loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8276",
   "metadata": {},
   "source": [
    "## 3. Train & Evaluate Across All Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73886d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MarketPulseEvaluator(num_classes=3)\n",
    "fold_results = []\n",
    "models = []\n",
    "\n",
    "for fold in folds:\n",
    "    X_train, y_train, X_test, y_test = validator.get_fold_data(X, y, fold)\n",
    "    \n",
    "    # Train\n",
    "    model = MarketPulseXGBClassifier.from_strategy_config(strategy_config)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    result = evaluator.evaluate_fold(\n",
    "        y_true=y_test.values.astype(int),\n",
    "        y_pred=y_pred,\n",
    "        y_proba=y_proba,\n",
    "        fold_number=fold.fold_number,\n",
    "        train_size=fold.train_size,\n",
    "        test_start_date=fold.test_start_date,\n",
    "        test_end_date=fold.test_end_date,\n",
    "    )\n",
    "    fold_results.append(result)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold.fold_number:2d}: acc={result.accuracy:.3f}  f1={result.f1:.3f}  \"\n",
    "          f\"[train={fold.train_size}, test={fold.test_size}]\")\n",
    "\n",
    "# Aggregate\n",
    "feature_importance = models[-1].get_feature_importance()\n",
    "report = evaluator.aggregate_results(\n",
    "    fold_results, ticker='AAPL', strategy='short_term',\n",
    "    horizon=1, feature_importance=feature_importance\n",
    ")\n",
    "_ = evaluator.print_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af68d2b",
   "metadata": {},
   "source": [
    "## 4. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437046b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy over time\n",
    "fig = evaluator.plot_fold_accuracy(report)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig = evaluator.plot_confusion_matrix(report)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "fig = evaluator.plot_feature_importance(report, top_n=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2588b3b",
   "metadata": {},
   "source": [
    "## 5. SHAP Explainability Analysis\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) shows **why** the model makes each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last trained model and last test fold\n",
    "last_model = models[-1]\n",
    "last_fold = folds[-1]\n",
    "X_train_last, _, X_test_last, _ = validator.get_fold_data(X, y, last_fold)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = last_model.get_shap_values(X_test_last)\n",
    "\n",
    "# For multi-class, shap_values is a list of arrays (one per class)\n",
    "# Show SHAP for class 2 (UP)\n",
    "print(\"SHAP summary for UP predictions:\")\n",
    "if isinstance(shap_values, list):\n",
    "    shap.summary_plot(shap_values[2], X_test_last, plot_type='bar', show=False)\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_test_last, plot_type='bar', show=False)\n",
    "plt.title('SHAP Feature Importance — UP Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39999416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed SHAP beeswarm for UP class\n",
    "print(\"SHAP beeswarm — how each feature pushes predictions:\")\n",
    "if isinstance(shap_values, list):\n",
    "    shap.summary_plot(shap_values[2], X_test_last, show=False)\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_test_last, show=False)\n",
    "plt.title('SHAP Beeswarm — UP Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574b62a",
   "metadata": {},
   "source": [
    "## 6. Horizon Comparison\n",
    "\n",
    "How does prediction accuracy change for 1-day vs 3-day vs 5-day horizons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_results = {}\n",
    "\n",
    "for horizon in [1, 3, 5]:\n",
    "    # Regenerate labels for this horizon\n",
    "    df_h = df.drop(columns=[c for c in df.columns if c.startswith('fwd_return') or c in ['label', 'label_name']], errors='ignore')\n",
    "    df_h = generate_labels(df_h, horizon=horizon, label_type='classification', num_classes=3, threshold=0.01)\n",
    "    X_h, y_h = get_clean_features_and_labels(df_h)\n",
    "    \n",
    "    folds_h = validator.split(X_h)\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    \n",
    "    for fold in folds_h:\n",
    "        X_tr, y_tr, X_te, y_te = validator.get_fold_data(X_h, y_h, fold)\n",
    "        m = MarketPulseXGBClassifier.from_strategy_config(strategy_config)\n",
    "        m.fit(X_tr, y_tr)\n",
    "        preds = m.predict(X_te)\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "        accs.append(accuracy_score(y_te.astype(int), preds))\n",
    "        f1s.append(f1_score(y_te.astype(int), preds, average='weighted', zero_division=0))\n",
    "    \n",
    "    horizon_results[horizon] = {\n",
    "        'accuracy': np.mean(accs),\n",
    "        'accuracy_std': np.std(accs),\n",
    "        'f1': np.mean(f1s),\n",
    "        'f1_std': np.std(f1s),\n",
    "    }\n",
    "    print(f\"Horizon {horizon}d: acc={np.mean(accs):.3f}±{np.std(accs):.3f}, f1={np.mean(f1s):.3f}±{np.std(f1s):.3f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "horizons = list(horizon_results.keys())\n",
    "accs = [horizon_results[h]['accuracy'] for h in horizons]\n",
    "f1s = [horizon_results[h]['f1'] for h in horizons]\n",
    "acc_errs = [horizon_results[h]['accuracy_std'] for h in horizons]\n",
    "\n",
    "x = np.arange(len(horizons))\n",
    "ax.bar(x - 0.15, accs, 0.3, yerr=acc_errs, label='Accuracy', color='steelblue', capsize=5)\n",
    "ax.bar(x + 0.15, f1s, 0.3, label='F1 Score', color='coral', capsize=5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{h}-day' for h in horizons])\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance by Prediction Horizon')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c0d81",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Walk-forward validation** is essential — it prevents the illusion of accuracy from data leakage.\n",
    "2. **3-class prediction** (UP/FLAT/DOWN) is harder than binary — the FLAT class dominates (~55% of labels at ±1% threshold).\n",
    "3. **SHAP analysis** reveals which features actually drive predictions — typically ATR, momentum, and MACD features are most important.\n",
    "4. **Phase 1 baseline** is established. Improvements will come from: feature selection, hyperparameter tuning, and sentiment data (Phase 2).\n",
    "\n",
    "Next: Notebook 04 — Clustering Analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
